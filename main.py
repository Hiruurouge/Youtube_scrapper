# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VGEt6Td8PMfWYJfUYXHcJDD1xZP-VMuz
"""

import requests
from bs4 import BeautifulSoup
import json
import re
import unicodedata
import sys
import getopt
import time
from selenium.webdriver import Chrome
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

def get_url(inputfile):
    f = open(inputfile);
    data = json.load(f)
    url_table = []
    for i in data['videos_id']:
        url_table.append('https://youtu.be/'+i)
    return (url_table)


def scrap(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    body = soup.find_all("body")[0]
    scripts = body.find_all("script")
    result = json.loads(scripts[0].string[30:-1])

    Video_details = result['videoDetails']

    title = soup.find("meta", itemprop="name")['content']
    description = soup.find("meta", itemprop="description")['content']
    views = Video_details['viewCount']
    author = Video_details['author']
    id = Video_details['videoId']

    description = unicodedata.normalize(
        'NFKD', Video_details['shortDescription']).encode('ascii', 'ignore')
    description = str(description.decode())
    description = description.replace('\n', '')

    data = re.search(r"var ytInitialData = ({.*?});", soup.prettify()).group(1)
    data = json.loads(data)
    videoPrimaryInfoRenderer = data['contents']['twoColumnWatchNextResults'][
        'results']['results']['contents'][0]['videoPrimaryInfoRenderer']
    likes = videoPrimaryInfoRenderer['videoActions']['menuRenderer']['topLevelButtons'][0][
        'segmentedLikeDislikeButtonRenderer']['likeButton']['toggleButtonRenderer']['defaultText']['simpleText']

    comm=[]

    with Chrome(executable_path=r'~/chromedriver_linux64/chromedriver') as driver:
        wait = WebDriverWait(driver,15)
        driver.get(url)

        for item in range(2): 
            wait.until(EC.visibility_of_element_located((By.TAG_NAME, "body"))).send_keys(Keys.END)
            time.sleep(3)

        for comment in wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "#content"))):
            comm.append(comment.text)
    final = data = {"title": title,
                   "views": views,
                   "likes": likes,
                   "description": description,
                   "author": author,
                   "id": id,
                    "comm":comm}
    return final


def generate_json(url_table,outputfile):
    f = open("output.json", "w")
    for i in range(len(url_table)):
        if i == 0:
            f.write('[')
        json.dump(scrap(url_table[i]), f)
        if i != len(url_table)-1:
            f.write(',')
        if i == len(url_table)-1:
            f.write(']')
    f.close()
    return 0


def main(argv):
    inputfile = ''
    outputfile = ''
    try:
        opts, args = getopt.getopt(argv,"hi:o:",["inputfile=","outputfile="])
    except getopt.GetoptError:
      print('test.py -i <inputfile> -o <outputfile>')
      sys.exit(2)
    for opt, arg in opts:
        if opt == '-h':
            print('test.py -i <inputfile> -o <outputfile>')
            sys.exit()
        elif opt in ("-i", "--inputfile"):
            inputfile = arg
        elif opt in ("-o", "--outputfile"):
            outputfile = arg    
    url_table=get_url(inputfile)
    generate_json(url_table,outputfile)
if __name__ == "__main__":
    main(sys.argv[1:])